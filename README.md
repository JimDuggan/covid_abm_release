
# README

This repository contains code to run experiments from the paper:

Wilder, Bryan and Charpignon, Marie and Killian, Jackson and Ou, Han-Ching and Mate, Aditya and Jabbari, Shahin and Perrault, Andrew and Desai, Angel and Tambe, Milind and Majumder, Maimuna, Modeling between-population variation in COVID-19 dynamics in Hubei, Lombardy, and New York City (March 31, 2020). Available at SSRN: [https://ssrn.com/abstract=3564800](https://ssrn.com/abstract=3564800) or [http://dx.doi.org/10.2139/ssrn.3564800](https://dx.doi.org/10.2139/ssrn.3564800)

## Main files to run the model and reproduce the results in the paper

* seir_individual.py: contains the main simulation logic
* run_experiments.py: calls seir_individual to run the experiment with the given parameter settings. Parameter settings are specified in the .json files in the inputs/ folder. jsons are included to run all of the experiments from the paper.
* analyze_bayesian.py: compute the posterior distribution from stored simulation results (e.g., after running the simulation on nyc_bayesian.json). outputs validation plots as in Fig 2 and Figs S1-S3. Also outputs a .pickle file with samples from the posterior distribution, which is needed as input to run the policy experiments. 
* analyze_bayesian_comparative.py: generates histograms of the posterior density with r0 and the documentation rate for each location.
* analyze_bayesian_policy.py: generates plots comparing the results of different policies.

Example:

python run_experiments.py --N_SIMS_PER_COMBO 1 --N_SIMS_PER_JOB 1 --sim_name lombardy_bayesian --popdir <path to population files> --EXP_ROOT_DIR <desired output directory> -N 10000000 -t 4 --index 0 --seed_offset 0

will run a single sample of the experiments needed to produce the posterior distribution for Lombardy (reading the input file inputs/lombardy_bayesian.json), using 4 threads.

## Notes

* Files with synthetic populations are included for Hubei and Lombardy. Code to sample the individual population attributes can be found in sample_household.py and sample_comorbidities.py
* For New York City, we use census microdata from [IPUMS](https://international.ipums.org/international/). Researchers can apply for access. sample_population_microdata.py can be be used to generate the NYC population (see the file for which microdata fields are needed). 
* The baseline infection fatality rates are generated by comorbidity_inference_ifr.py. The estimates from the paper are already included in the repo and read in automatically.

## Running large-scale experiments

We ran experiments on a slurm cluster and include the requisite scripts to launch the experiments and merge results. 

* The strategy for parallelization is two-fold. First, independent runs of the simulation can be performed by many jobs in parallel. Second, each individual simulation can utilize multiple cores. We use sbatch to launch independent runs of the simulation in parallel. For experiments aimed at obtaining the posterior distribution, each job draws independent samples from the prior and runs the simulation for each. For the policy experiments, each job runs on a different set of samples from the posterior.
* The files combine_bayesian.py and combine_bayesian_policy.py are used to collect results from runs that split computation over several independent jobs.
* We include an example of a .sh file that launches an experiment for Lombardy. This script could be run with 

sbatch --array=0-100 job.example_lombardy.sh lombardy_bayesian 10 10 0

in order to run the scenario described in inputs/lombardy_bayesian.json. It will launch 100 jobs, each of which performs 10 runs of the simulation. The results can then be merged with 

python combine_bayesian.py --json inputs/lombardy_bayesian.json --njobs 100

* The script launch_all_policy.py automates launching all of the policy experiments for a given country. combine_bayesian_policy.py should then be used to merge the results.
* We only include one example .sh file because these will have to be customized to your environment.